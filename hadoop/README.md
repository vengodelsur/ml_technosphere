## Курс "Методы распределенной обработки больших объемов данных в Hadoop"
[Программа курса на сайте Техносферы@Mail.Ru](https://sphere.mail.ru/curriculum/program/discipline/816/)

`hadoop/`

### InputFormat
`inputformat/`
Имеется дамп страниц новостного сайта. Ваша задача - реализовав собственный InputFormat, посчитать, в скольких документах встречается каждое слово. Реальное применение задачи - подсчет IDF по новостному корпусу.

Формат входных данных: сжатые с помощью deflate страницы находятся в файле .pkz. Как вы помните из лекции, такие файлы не разделяются, т.к. deflate не имеет зарезервированных последовательностей. Поэтому, отдельно расположен файл .pkz.idx, представляющий из себя размеры каждой записи [int, 4 байта].

### SEO (композитный ключ)
`hw2-clusters/`
Вам даны пары <запрос, URL>, вам необходимо найти лучший запрос к каждому хосту. Напомню что, хост эта часть схемы URL: протокол://хост[:порт]/путь?параметры. Напомню, Hadoop не сортирует значения, и для решения "в лоб" нам необходимо держать все запросы соответствующие данному хосту в памяти. И разнообразие запросов к определенным хостам, таким как ru.wikipedia.org или otvet.mail.ru может сделать это проблемным. Поэтому необходимо использовать композитный ключ.

