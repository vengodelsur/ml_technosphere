{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import io\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(file_name):\n",
    "    with open(file_name, 'r') as f:\n",
    "        loaded = pickle.load(f)\n",
    "    return loaded\n",
    "\n",
    "def dump(obj, file_name):\n",
    "    with open(file_name, 'wb') as f:\n",
    "        pickle.dump(obj, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with io.open('train_data.json','r',encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        d = json.loads(line)\n",
    "        train_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "with io.open('test_data.json','r',encoding='utf8') as f:\n",
    "    for line in f.readlines():\n",
    "        d = json.loads(line)\n",
    "        test_data.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract marks from train (true ends and potential ones)\n",
    "symbols = {u'!', u'\"', u'.', u'?', u'\\xbb', u'\\u2026'}\n",
    "\n",
    "def find_occurences(string, symbols):\n",
    "    return [i for i, letter in enumerate(string) if letter in symbols]\n",
    "\n",
    "all_marks = []\n",
    "for j, item in enumerate(train_data):\n",
    "    sentences = train_data[j][u'Sentences']\n",
    "    paragraph = train_data[j][u'Paragraph']\n",
    "    number_of_sentences = len(sentences)\n",
    "    lengths = [len(sentences[i]) for i in range(number_of_sentences)]\n",
    "    end_positions = np.cumsum(lengths) + [i - 1 for i in range(number_of_sentences)]\n",
    "    end_symbols = [paragraph[i] for i in end_positions]\n",
    "    # real_end_symbols = [sentences[i][-1] for i in range(number_of_sentences)]\n",
    "    potential_end_positions = find_occurences(paragraph, symbols)\n",
    "    all_marks.append((paragraph, end_positions, potential_end_positions))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_marks_to_test_format(all_marks):\n",
    "\n",
    "    index = 0\n",
    "    bool_by_index = {}\n",
    "    all_paragraphs = []\n",
    "\n",
    "    for item in all_marks:\n",
    "        paragraph = item[0]\n",
    "        end_mark_positions = item[1]\n",
    "        mark_positions = item[2]\n",
    "        paragraph_with_marks = {u'Marks': [], u'Paragraph': paragraph}\n",
    "    \n",
    "        for pos in mark_positions:\n",
    "            index +=1\n",
    "            bool_by_index[index] = (pos in end_mark_positions)\n",
    "            paragraph_with_marks[u'Marks'].append({u'Index': index, u'Mark': paragraph[pos], u'Pos': pos})\n",
    "        \n",
    "        all_paragraphs.append(paragraph_with_marks)\n",
    "        \n",
    "    return all_paragraphs, bool_by_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_marks_in_test_format, bool_by_index_train = train_marks_to_test_format(all_marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_format_to_df(data_list):\n",
    "    list_of_dictionaries = []\n",
    "    for i, item in enumerate(data_list):\n",
    "        for mark in item[u'Marks']:\n",
    "            d = mark\n",
    "            d[u'Paragraph_id'] = i\n",
    "            d['Paragraph'] = item[u'Paragraph']\n",
    "            list_of_dictionaries.append(d)\n",
    "            \n",
    "    paragraphs_df = pd.DataFrame(list_of_dictionaries)\n",
    "    \n",
    "    return paragraphs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = test_format_to_df(train_marks_in_test_format)\n",
    "train_df['is_end'] = bool_by_index_train\n",
    "test_df = test_format_to_df(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фичи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fives(df):\n",
    "    next_five = []\n",
    "    last_five = []\n",
    "    given = df[['Pos', 'Paragraph']]\n",
    "    for index, row in given.iterrows():\n",
    "        paragraph = row['Paragraph']\n",
    "        length = len(paragraph)\n",
    "        pos = row['Pos']\n",
    "        last_five.append(paragraph[max(0, pos-5):pos])\n",
    "        next_five.append(paragraph[pos+1:min(pos+1+5, length)])\n",
    "    df['last_five'] = last_five\n",
    "    df['next_five'] = next_five\n",
    "    df.loc[df['next_five'] == '', 'next_five'] = np.nan\n",
    "    df.loc[df['last_five'] == '', 'last_five'] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = add_fives(train_df)\n",
    "test_df = add_fives(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scheme(series):\n",
    "    capital_cyrillic = u'[ЙЦУКЕНГШЩЗХЪЁФЫВАПРОЛДЖЭЯЧСМИТЬБЮ]'\n",
    "    cyrillic = u'[йцукенгшщзхъёфывапролджэячсмитьбюўѢ]'\n",
    "    chinese = u'[⺀-⺙⺛-⻳⼀-⿕々〇〡-〩〸-〺〻㐀-䶵一-鿃豈-鶴侮-頻並-龎]'\n",
    "    japanese = u'[\\u3000-\\u303f\\u3040-\\u309f\\u30a0-\\u30ff\\uff00-\\uff9f\\u4e00-\\u9faf\\u3400-\\u4dbf]'\n",
    "    capital_greek = u'[ΑΓΒΕΔΗΖΙΘΛΚΝΜΟΞΡΠΣΥΤΧΦΩΨ]'\n",
    "    greek = u'[αγβεδηζιθλκνμοξρπσςυὐύτχφωψὰѲῶᾠέάἀὀ]'\n",
    "    arabic = u'[\\u0627-\\u064a]'\n",
    "    hebrew = u'[־-״]'\n",
    "    korean = u'[천하\\u3131-\\ucb4c]'\n",
    "    almost_latin = u'[ßáàãâǎåäçčéèëêíìἰïĭόіїɪǐīîñóòõōôöùūøûúýüÿńšəðІʀἙἘɣ]'\n",
    "    capital_almost_latin = u'[ÁÀÃÂÅÄÇÉÈËÊÍÌÏÎÑÓÒÕÔÖÙØÛÚÝÜ]'\n",
    "    math = u'[±×·¬\\+]'\n",
    "    digits = u'[³²¹¾½¼∞]'\n",
    "    strange = u'[গअোপदԿावাխ]'\n",
    "   \n",
    "    schemed_series = series.replace(np.nan, '')\n",
    "    \n",
    "    schemed_series = schemed_series.str.replace(capital_cyrillic, u'К')\n",
    "    schemed_series = schemed_series.str.replace(cyrillic, u'к')\n",
    "    schemed_series = schemed_series.str.replace('[A-Z]', 'L')\n",
    "    schemed_series = schemed_series.str.replace('[a-z]', 'l')\n",
    "    schemed_series = schemed_series.str.replace(capital_almost_latin, 'L')\n",
    "    schemed_series = schemed_series.str.replace(almost_latin, 'l')\n",
    "    schemed_series = schemed_series.str.replace(chinese, 'c')\n",
    "    schemed_series = schemed_series.str.replace(japanese, 'j')\n",
    "    schemed_series = schemed_series.str.replace(arabic, 'a')\n",
    "    schemed_series = schemed_series.str.replace(capital_greek, u'G')\n",
    "    schemed_series = schemed_series.str.replace(greek, u'g')\n",
    "    schemed_series = schemed_series.str.replace(korean, u'k')\n",
    "    schemed_series = schemed_series.str.replace(hebrew, u'h')\n",
    "    schemed_series = schemed_series.str.replace(math, 'm')\n",
    "    schemed_series = schemed_series.str.replace(strange, 's')\n",
    "    schemed_series = schemed_series.str.replace('[0-9]', '0')\n",
    "    schemed_series = schemed_series.str.replace(digits, '0')\n",
    "    \n",
    "    #schemed_series = series.replace('', np.nan)\n",
    "    \n",
    "    \n",
    "    return schemed_series\n",
    "\n",
    "def df_scheme(df):\n",
    "    df['next_five_scheme'] = scheme(df['next_five'])\n",
    "    df['last_five_scheme'] = scheme(df['last_five'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df_scheme(train_df)\n",
    "test_df = df_scheme(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_schemes(df):\n",
    "    df['next_four_scheme'] = df['next_five_scheme'].str.slice(0,4)\n",
    "    df['next_three_scheme'] = df['next_five_scheme'].str.slice(0,3)\n",
    "    df['next_two_scheme'] = df['next_five_scheme'].str.slice(0,2)\n",
    "    df['next_one_scheme'] = df['next_five_scheme'].str.slice(0,1)\n",
    "    \n",
    "    df['last_one_scheme'] = df['last_five_scheme'].str.slice(4,5)\n",
    "    df['last_two_scheme'] = df['last_five_scheme'].str.slice(3,5)\n",
    "    df['last_three_scheme'] = df['last_five_scheme'].str.slice(2,5)\n",
    "    df['last_four_scheme'] = df['last_five_scheme'].str.slice(1,5)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = add_schemes(test_df)\n",
    "train_df = add_schemes(train_df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemes = ['next_five_scheme','next_four_scheme', 'next_three_scheme','next_two_scheme','next_one_scheme','last_one_scheme', 'last_two_scheme', 'last_five_scheme', 'last_three_scheme', 'last_five_scheme', 'last_four_scheme','last_five_scheme']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ohe = pd.get_dummies(test_df, columns=schemes)\n",
    "train_df_ohe = pd.get_dummies(train_df, columns=schemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_double_columns(df):\n",
    "    df = df.loc[:, ~df.columns.duplicated()]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_ohe = delete_double_columns(test_df_ohe)\n",
    "train_df_ohe = delete_double_columns(train_df_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ohe_features = test_df_ohe.columns[7:]\n",
    "train_ohe_features = train_df_ohe.columns[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_frequent_features(df_ohe, ohe_features):\n",
    "    freq_ohe_features = df_ohe[ohe_features].sum()>1\n",
    "    return freq_ohe_features[freq_ohe_features==True].index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_test_ohe_features = extract_frequent_features(test_df_ohe, test_ohe_features)\n",
    "freq_train_ohe_features = extract_frequent_features(train_df_ohe, train_ohe_features)\n",
    "common_freq_features = set(freq_train_ohe_features).intersection(set(freq_test_ohe_features))\n",
    "common_freq_features = list(common_freq_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_features = [u'Mark_!', u'Mark_\"', u'Mark_.', u'Mark_?', u'Mark_»', u'Mark_…', u'is_last_mark', u'is_first_mark', u'only_mark', u'distance_to_next', u'distance_to_last', u'beginning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ohe_mark_features(df):\n",
    "    new_df = pd.get_dummies(df, columns=['Mark'])\n",
    "    new_df['Mark'] = df['Mark']\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "def add_paragraph_boundaries(df):\n",
    "    df['next_id'] = df['Paragraph_id'].shift(-1)\n",
    "    df['last_id'] = df['Paragraph_id'].shift(+1)\n",
    "    \n",
    "    df['is_last_mark'] = (df['Paragraph_id'] != df['next_id'])\n",
    "    df['is_first_mark'] = (df['Paragraph_id'] != df['last_id'])\n",
    "    \n",
    "    df['only_mark'] = df['is_first_mark'] & df['is_last_mark']\n",
    "    \n",
    "    df = df.drop(['next_id', 'last_id'],axis=1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_distances(df):\n",
    "    df['distance_to_next'] = df['Pos'].shift(-1) - df['Pos']\n",
    "    df.loc[(df['is_last_mark']==1),['distance_to_next']] = np.inf\n",
    "    \n",
    "    df['distance_to_last'] = df['Pos'] - df['Pos'].shift(+1) \n",
    "    df.loc[(df['is_first_mark']==1),['distance_to_last']] = np.inf\n",
    "    \n",
    "    return df\n",
    "\n",
    "def next_last_distance_max(df, train_df):\n",
    "    max_distance_to_next_train = train_df['distance_to_next'].replace(np.inf, np.nan).max()\n",
    "    max_distance_to_next = df['distance_to_next'].replace(np.inf, np.nan).max()\n",
    "    max_distance_to_next = max(max_distance_to_next_train, max_distance_to_next)\n",
    "    \n",
    "    max_distance_to_last_train = train_df['distance_to_last'].replace(np.inf, np.nan).max()\n",
    "    max_distance_to_last = df['distance_to_last'].replace(np.inf, np.nan).max()\n",
    "    max_distance_to_last = max(max_distance_to_last_train, max_distance_to_last)\n",
    "    \n",
    "    return max_distance_to_next, max_distance_to_last\n",
    "\n",
    "def replace_inf_in_column(df, column, value):\n",
    "    df[column] = df[column].replace(np.inf, value)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def add_elementary_features(df):\n",
    "    df['beginning'] = (df['Pos'] <=1)\n",
    "    df = add_paragraph_boundaries(df)\n",
    "    df = add_distances(df)\n",
    "    df = add_ohe_mark_features(df)\n",
    "    return df\n",
    "\n",
    "def add_plain_features(train_df, test_df):\n",
    "    \n",
    "    train_df = add_elementary_features(train_df)\n",
    "    test_df = add_elementary_features(test_df)    \n",
    "       \n",
    "    next_max, last_max = next_last_distance_max(test_df, train_df)\n",
    "    \n",
    "    for df in ([train_df, test_df]):\n",
    "        df = replace_inf_in_column(df, 'distance_to_next', next_max)\n",
    "        df = replace_inf_in_column(df, 'distance_to_last', last_max)\n",
    "    \n",
    "    \n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_ohe, test_df_ohe = add_plain_features(train_df_ohe, test_df_ohe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_features = {'next_five_scheme_', 'next_four_scheme_', 'next_three_scheme_', \n",
    "                'next_two_scheme_', 'next_one_scheme_', 'last_one_scheme_',\n",
    "                'last_two_scheme_', 'last_three_scheme_', 'last_four_scheme_',\n",
    "                'last_five_scheme_'}\n",
    "features = list(set(plain_features + common_freq_features) - nan_features) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, я не зафиксировала random state, но запиклила порядок фичей и получившийся лес. \n",
    "Поэтому использовавшийся код переведен в markdown и заменен запикленной моделью. Надеюсь, это не страшно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_freq_features = load('common_freq_features.pkl')\n",
    "features = list(set(plain_features + common_freq_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "clf.fit(train_df_ohe[features].values, train_df_ohe['is_end'].astype(int).values) \n",
    "\n",
    "test_df_ohe['rf_predicted'] = clf.predict(test_df_ohe[features].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = load('rfclassifier.pkl')\n",
    "test_df_ohe['rf_predicted'] = clf.predict(test_df_ohe[features].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Добавляем эвристики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_predict(df):\n",
    "    df['predicted'] = df['rf_predicted']\n",
    "    df.loc[df['ok_ending'] == True, 'predicted'] = True\n",
    "    df.loc[df['problematic_abbreviation'] == True, 'predicted'] = False\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_problematic_abbreviations(df):\n",
    "    te = df['last_five'].str.contains(u'т\\. ?[ке]$', regex=True)\n",
    "    \n",
    "    languages = [u' англ', u' рус', u' рум', u' лат', u' яп', u' кит', u'(англ', u'(рус', u'(рум', u'(лат', u'(яп', u'(кит']\n",
    "    language = pd.Series(False, index = df.index)\n",
    "    for lang in languages:\n",
    "        language = language | df['last_five'].str.endswith(lang)\n",
    "        \n",
    "    red = df['last_five'].str.contains(u'[ (\\.]ред$', regex=True)\n",
    "    sm = df['last_five'].str.contains(u'[ (]См$', regex=True)\n",
    "    napr = df['last_five'].str.contains(u'[ (]напр$', regex=True)   \n",
    "    pic = df['last_five'].str.contains(u'[ (]рис$', regex=True) & df['next_five'].str.contains(u'^\\d+', regex=True)\n",
    "    \n",
    "    cityname = u'^ ?[ЙЦУКЕНГШЩЗХЪЁФЫВАПРОЛДЖЭЯЧСМИТЬБЮ]'\n",
    "    city = df['last_five'].str.endswith(u'в г') & df['next_five'].str.contains(cityname, regex=True)\n",
    "    \n",
    "    df['problematic_abbreviation'] = te | language | red | sm | napr | pic | city\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def add_ok_endings(df):\n",
    "    new_sentence = df['next_five'].str.contains(u'^ [ЙЦУКЕНГШЩЗХЪЁФЫВАПРОЛДЖЭЯЧСМИТЬБЮ]', regex=True)\n",
    "    \n",
    "    recipe = df['last_five'].str.contains(u' ?\\d+г$', regex=True) & new_sentence\n",
    "    volume = df['last_five'].str.contains(u' ?Т. ?\\d+$', regex=True) & new_sentence\n",
    "    quote = df['last_five'].str.endswith(u'»') & new_sentence & df['Mark_.']\n",
    "    \n",
    "    df['ok_ending'] = recipe | volume | quote\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = add_problematic_abbreviations(test_df)\n",
    "train_df = add_problematic_abbreviations(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = add_plain_features(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = add_ok_endings(test_df)\n",
    "train_df = add_ok_endings(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['rf_predicted'] = pd.read_csv('submission_rf_freq_schemes.csv')['Mark'] # comment to use computed rf_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = rf_predict(test_df)\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['Id'] = test_df['Index']\n",
    "submission['Mark'] = test_df['predicted'].astype(int)\n",
    "submission.to_csv('solution_Kuznetsova.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
